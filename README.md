# ğŸ“š From Paper to Code: Implementations of Key Computer Vision Papers from Scratch

Welcome to my computer vision model implementation series!  
This repository serves as a central hub for my **from-scratch implementations** of legendary computer vision models, each broken down and reconstructed with clarity and precision.



---

## ğŸš€ Introduction

This repository briefly introduces my **legendary computer vision model implementation series**, where I faithfully reimplement influential papers to gain hands-on, low-level understanding of the techniques driving modern visual AI systems.

---

## ğŸ¯ Motivation

The purpose of this project is twofold:

1. **Develop Practical Proficiency**: To gain end-to-end experience in building deep learning pipelines â€” from data preparation to training, inference, and deployment â€” in the domain of computer vision.
2. **Build Foundational Understanding**: To deeply understand the core architectural paradigms of modern vision models, such as **Convolutional Neural Networks (CNNs)** and **Transformers**.

---

## ğŸ” Workflow

Each implementation follows a consistent and rigorous pipeline:

![image](https://github.com/user-attachments/assets/c0bb7f29-2b8b-474e-8ce6-7a8fcc2cdee1)

1. **Paper Reading & Understanding**  
   Analyze the original research paper to understand the theoretical backbone of the model.

2. **Data Preparation**  
   Prepare the dataset according to the specific requirements of the model.

3. **Model Building**  
   Implement the architecture from scratch using PyTorch.

4. **Training & Experimentation**  
   Train the model, tune hyperparameters, and analyze performance metrics.

5. **Inference & Deployment**  
   Evaluate the model with test samples and explore basic deployment options.

---

## ğŸ§  Implemented Models

### ğŸ“Œ YOLOv1 â€“ You Only Look Once (v1)  
ğŸ”— [GitHub Repository](https://github.com/bskkimm/Simple-YOLO-v1-implementation-from-scratch-using-PyTorch-) | [Medium Article](https://medium.com/@bskkim2022/yolo-v1-implementation-from-scratch-using-pytorch-d7fa95ff06ea)

YOLOv1 is a real-time object detection model that reframes detection as a single regression problem, directly predicting class probabilities and bounding boxes from full images in one evaluation. It introduced a new paradigm compared to two-stage detectors like R-CNN.

---

### ğŸ“Œ ViT â€“ Vision Transformer
ğŸ”— [GitHub Repository](https://github.com/bskkimm/Simple-ViT-Implementation) | [Medium Article](https://medium.com/@bskkim2022/paper-reimplementation-vit-vision-transformer-eed3ad20dfe7)

ViT replaces convolutional layers with pure transformer blocks. It splits the image into patches, flattens them, and feeds them into a standard transformer architecture, achieving state-of-the-art performance with sufficient data.


---

### ğŸ“Œ DETR â€“ DEtection TRansformer
ğŸ”— [GitHub Repository](https://github.com/bskkimm/Simple-DETR-Implementation) | [Medium Article](https://medium.com/@bskkim2022/detr-implementation-from-scratch-using-pytorch-0f783fe06363)

DETR is a novel object detection model that unifies detection with transformers and bipartite matching. It eliminates the need for many hand-crafted components like anchor boxes or NMS, making detection elegantly simple.

---

## ğŸ¤ Contributing

This is a solo learning project, but feel free to:
- Star â­ the repos
- Open issues for suggestions
- Fork and experiment


